# FLY-EVAL++ 论文就绪检查清单

**基于用户反馈的完整任务清单**

---

## ✅ 已完成（安全风险修复）

### 安全-1: 移除明文API Key ✅
- ✅ `run_full_evaluation_llm_judge.py` - 改为从环境变量读取
- ✅ `test_llm_judge_with_real_data.py` - 改为从环境变量读取
- ✅ `llm_judge.py` - 已支持从环境变量读取（无需修改）
- ⚠️ 文档中仍有API Key引用，需要清理（但这是文档，不是代码）

### 安全-2: Reasoning字段结构化 ✅
- ✅ `JudgeOutput.reasoning` - 从`str`改为`Dict[str, str]`（按维度）
- ✅ Prompt模板 - 更新为结构化reasoning要求
- ⚠️ 需要更新解析和验证逻辑（待完成）

---

## 🔴 P0 - 必须完成（论文投稿前）

### P0-1: 全模型评估（21个模型，S1/M1/M3，每任务≥100样本）
**状态**: ⏳ 待开始  
**优先级**: 🔴 最高

**任务**:
- [ ] 运行所有21个模型的评估
- [ ] 覆盖S1/M1/M3三个任务
- [ ] 每个任务至少100个样本（或全量708）
- [ ] 生成完整的评估结果文件

**预计时间**: 取决于API调用速度（可能需要数小时）

**脚本**: `run_full_evaluation_llm_judge.py`

---

### P0-2: 生成LaTeX主表+附表
**状态**: ⏳ 待开始  
**优先级**: 🔴 最高

**任务**:
- [ ] 主表：总体分、维度分（5个维度）
- [ ] 附表1：失败类型分布
- [ ] 附表2：尾部风险（P95/P99/超阈率）
- [ ] 附表3：约束满足率（按类型）
- [ ] 附表4：Eligibility率 vs Availability率

**脚本**: `generate_paper_results_llm_judge.py`（需要扩展）

---

### P0-3: 消融实验1 - Rule-based vs LLM-judge
**状态**: ⏳ 待开始  
**优先级**: 🔴 最高

**任务**:
- [ ] 使用相同样本集运行两种fusion方法
- [ ] 对比排序差异（Spearman correlation）
- [ ] 对比诊断能力（失败模式识别）
- [ ] 生成对比表格和可视化

**脚本**: 需要创建 `run_ablation_rule_vs_llm.py`

---

### P0-4: 消融实验2 - Evidence-only vs 含raw text
**状态**: ⏳ 待开始  
**优先级**: 🔴 最高

**任务**:
- [ ] 修改LLM Judge支持raw text输入（临时版本）
- [ ] 使用相同样本集运行两种输入方式
- [ ] 对比prompt injection敏感性
- [ ] 对比风格偏好影响
- [ ] 生成对比报告

**脚本**: 需要创建 `run_ablation_evidence_only.py`

---

### P0-5: 消融实验3 - Judge模型一致性测试
**状态**: ⏳ 待开始  
**优先级**: 🔴 最高

**任务**:
- [ ] 使用相同样本集运行多次（temperature=0）
- [ ] 检查一致性（应该100%一致）
- [ ] 如果发现不一致，分析原因（缓存、API问题等）
- [ ] 报告一致性统计

**脚本**: 需要创建 `run_ablation_consistency.py`

---

## 🟡 P1 - 强烈建议（论文质量）

### P1-1: 生成Rubric LaTeX表格 ✅
**状态**: ✅ 已完成  
**优先级**: 🟡 高

**任务**:
- ✅ 生成5维度×4档的LaTeX表格
- ✅ 包含条件描述和分数映射
- ✅ 生成Protocol定义文档

**脚本**: `generate_rubric_latex_table.py`

---

### P1-2: 编写Protocol Definition
**状态**: ✅ 已完成（部分）  
**优先级**: 🟡 高

**任务**:
- ✅ 固定映射协议说明（LaTeX）
- ✅ 聚合方式说明（算术平均）
- ✅ 单调性约束说明
- ⚠️ 需要整合到主论文中

**脚本**: `generate_rubric_latex_table.py`

---

### P1-3: 生成可靠性统计
**状态**: ⏳ 待开始  
**优先级**: 🟡 高

**任务**:
- [ ] Fallback率统计
- [ ] 校验失败率（单调性、evidence引用）
- [ ] 重试次数分布
- [ ] 平均token数/耗时
- [ ] API调用成功率

**脚本**: 需要扩展 `run_full_evaluation_llm_judge.py`

---

### P1-4: 阈值/约束强度Sanity Check
**状态**: ⏳ 待开始  
**优先级**: 🟡 高

**任务**:
- [ ] 分析"Physics/Cross-field全部D"现象
- [ ] 检查阈值是否过严
- [ ] 如果过严，解释原因（安全关键领域需要严格约束）
- [ ] 如果不过严，说明这是合理的（模型确实违反物理约束）

**脚本**: 需要创建 `analyze_constraint_thresholds.py`

---

## 📋 实施计划

### 第一阶段：安全修复（已完成）
- ✅ 移除API Key硬编码
- ✅ Reasoning字段结构化

### 第二阶段：P0任务（进行中）
1. **P0-1**: 全模型评估（最重要）
   - 先运行小规模测试（1-2个模型，10-20样本）
   - 确认无误后运行全量
   
2. **P0-2**: 生成LaTeX表格
   - 基于P0-1的结果生成
   
3. **P0-3/4/5**: 消融实验
   - 可以并行进行
   - 每个实验需要独立脚本

### 第三阶段：P1任务
- P1-1/1-2已完成
- P1-3/1-4需要基于P0-1的结果

---

## 🎯 当前状态总结

### ✅ 已完成
1. LLM Judge核心实现（evidence-only, rubric驱动）
2. LLM-Based Fusion（固定映射协议）
3. 安全风险修复（API Key, Reasoning字段）
4. Rubric LaTeX表格生成
5. Protocol定义文档

### ⏳ 进行中
1. 全模型评估脚本（需要测试和扩展）
2. LaTeX表格生成脚本（需要扩展）

### 📝 待开始
1. 全模型评估运行（21个模型，S1/M1/M3）
2. 消融实验（3组）
3. 可靠性统计
4. 约束强度分析

---

## ⚠️ 关键提醒

1. **API Key**: 所有脚本现在使用环境变量，运行前需要设置：
   ```bash
   export OPENAI_API_KEY="your-key"
   export OPENAI_API_BASE="https://xiaohumini.site/v1"
   ```

2. **成本估算**: 全模型评估（21模型 × 3任务 × 100样本 = 6300次LLM调用）
   - 如果使用gpt-4o，成本可能较高
   - 建议先小规模测试，确认无误后再全量运行

3. **时间估算**: 
   - 全模型评估：可能需要数小时到一天（取决于API速度）
   - 消融实验：每个实验可能需要数小时

4. **结果保存**: 所有结果必须保存到固定目录，并记录版本信息

---

**最后更新**: 2025-01-19  
**状态**: P0任务准备中，P1部分完成

