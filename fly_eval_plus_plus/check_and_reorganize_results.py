#!/usr/bin/env python3
"""
æ£€æŸ¥å¹¶é‡æ–°ç»„ç»‡è¯„ä¼°ç»“æœ
æŒ‰ç…§"è¾“å‡ºæ–‡ä»¶å¤¹-ä»»åŠ¡ç±»å‹S1M1M3-æ¨¡å‹ç›®å½•-æ¨¡å‹ç»“æœ"çš„æ ¼å¼æ•´åˆç»“æœ
"""

import os
import json
import shutil
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from fly_eval_plus_plus.core.data_structures import TaskSummary, ModelProfile


def check_results_quality(records_file: str, task_summary_file: str, model_profiles_file: str):
    """æ£€æŸ¥ç»“æœè´¨é‡"""
    print("=" * 80)
    print("æ£€æŸ¥ç»“æœè´¨é‡")
    print("=" * 80)
    
    # 1. æ£€æŸ¥è®°å½•æ–‡ä»¶
    print("\n1. æ£€æŸ¥è®°å½•æ–‡ä»¶...")
    with open(records_file, 'r', encoding='utf-8') as f:
        records = [json.loads(line) for line in f]
    
    print(f"   âœ… æ€»è®°å½•æ•°: {len(records)}")
    
    # ç»Ÿè®¡æ¨¡å‹å’Œæ ·æœ¬
    models = set(r['model_name'] for r in records)
    print(f"   âœ… æ¨¡å‹æ•°é‡: {len(models)}")
    print(f"   âœ… æ¨¡å‹åˆ—è¡¨: {sorted(models)[:5]}...")
    
    # ç»Ÿè®¡eligibleæ ·æœ¬
    eligible_count = sum(1 for r in records if r.get('agent_output', {}).get('adjudication') == 'eligible')
    print(f"   âœ… Eligibleæ ·æœ¬æ•°: {eligible_count}")
    print(f"   âœ… Eligibleç‡: {eligible_count / len(records) * 100:.2f}%")
    
    # æ£€æŸ¥è®°å½•å®Œæ•´æ€§
    required_keys = ['sample_id', 'model_name', 'task_id', 'protocol_result', 'evidence_pack', 'agent_output', 'optional_scores']
    incomplete_records = []
    for i, r in enumerate(records[:100]):  # åªæ£€æŸ¥å‰100æ¡
        missing_keys = [k for k in required_keys if k not in r]
        if missing_keys:
            incomplete_records.append((i, missing_keys))
    
    if incomplete_records:
        print(f"   âš ï¸  å‘ç° {len(incomplete_records)} æ¡ä¸å®Œæ•´è®°å½•ï¼ˆå‰100æ¡ä¸­ï¼‰")
        for idx, missing in incomplete_records[:5]:
            print(f"      è®°å½• {idx}: ç¼ºå°‘ {missing}")
    else:
        print("   âœ… è®°å½•å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
    
    # 2. æ£€æŸ¥ä»»åŠ¡æ‘˜è¦
    print("\n2. æ£€æŸ¥ä»»åŠ¡æ‘˜è¦...")
    if os.path.exists(task_summary_file):
        with open(task_summary_file, 'r', encoding='utf-8') as f:
            task_summary = json.load(f)
        
        print(f"   âœ… æ€»æ ·æœ¬æ•°: {task_summary.get('total_samples', 0)}")
        print(f"   âœ… Eligibleæ ·æœ¬æ•°: {task_summary.get('eligible_samples', 0)}")
        print(f"   âœ… Eligibleç‡: {task_summary.get('eligibility_rate', 0.0)}")
        
        # æ£€æŸ¥eligibility_rateæ˜¯å¦æ­£ç¡®
        expected_rate = (task_summary.get('eligible_samples', 0) / task_summary.get('total_samples', 1)) * 100
        actual_rate = task_summary.get('eligibility_rate', 0.0)
        if abs(expected_rate - actual_rate) > 0.01:
            print(f"   âš ï¸  Eligibilityç‡è®¡ç®—é”™è¯¯ï¼")
            print(f"      æœŸæœ›å€¼: {expected_rate:.2f}%")
            print(f"      å®é™…å€¼: {actual_rate:.2f}%")
            return False
        else:
            print("   âœ… Eligibilityç‡è®¡ç®—æ­£ç¡®")
    else:
        print(f"   âŒ ä»»åŠ¡æ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨: {task_summary_file}")
        return False
    
    # 3. æ£€æŸ¥æ¨¡å‹ç”»åƒ
    print("\n3. æ£€æŸ¥æ¨¡å‹ç”»åƒ...")
    if os.path.exists(model_profiles_file):
        try:
            with open(model_profiles_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    model_profiles = json.loads(content)
                else:
                    model_profiles = {}
        except (json.JSONDecodeError, ValueError) as e:
            print(f"   âš ï¸  æ¨¡å‹ç”»åƒæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            model_profiles = {}
        
        if not model_profiles:
            print(f"   âš ï¸  æ¨¡å‹ç”»åƒæ–‡ä»¶ä¸ºç©ºï¼Œå°†åœ¨é‡æ–°ç»„ç»‡æ—¶ä»è®°å½•ç”Ÿæˆ")
        else:
            print(f"   âœ… æ¨¡å‹ç”»åƒæ•°é‡: {len(model_profiles)}")
            print(f"   âœ… æ¨¡å‹åˆ—è¡¨: {sorted(model_profiles.keys())[:5]}...")
    else:
        print(f"   âš ï¸  æ¨¡å‹ç”»åƒæ–‡ä»¶ä¸å­˜åœ¨: {model_profiles_file}ï¼Œå°†åœ¨é‡æ–°ç»„ç»‡æ—¶ä»è®°å½•ç”Ÿæˆ")
    
    print("\n" + "=" * 80)
    print("âœ… ç»“æœè´¨é‡æ£€æŸ¥å®Œæˆ")
    print("=" * 80)
    
    return True


def reorganize_results(
    base_output_dir: str,
    task_id: str,
    records_file: str,
    task_summary_file: str,
    model_profiles_file: str
):
    """
    é‡æ–°ç»„ç»‡ç»“æœæ–‡ä»¶
    
    æŒ‰ç…§"è¾“å‡ºæ–‡ä»¶å¤¹-ä»»åŠ¡ç±»å‹-æ¨¡å‹ç›®å½•-æ¨¡å‹ç»“æœ"çš„æ ¼å¼ç»„ç»‡
    """
    print("\n" + "=" * 80)
    print(f"é‡æ–°ç»„ç»‡ç»“æœ - {task_id}")
    print("=" * 80)
    
    # åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„
    # base_output_dir/task_id/model_name/
    new_base_dir = Path(base_output_dir) / task_id
    new_base_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ‰€æœ‰è®°å½•
    print("\nğŸ“‚ åŠ è½½è®°å½•æ–‡ä»¶...")
    with open(records_file, 'r', encoding='utf-8') as f:
        records = [json.loads(line) for line in f]
    
    print(f"   âœ… åŠ è½½ {len(records)} æ¡è®°å½•")
    
    # æŒ‰æ¨¡å‹åˆ†ç»„
    records_by_model = defaultdict(list)
    for record in records:
        model_name = record['model_name']
        records_by_model[model_name].append(record)
    
    print(f"   âœ… åˆ†ç»„åˆ° {len(records_by_model)} ä¸ªæ¨¡å‹")
    
    # åŠ è½½ä»»åŠ¡æ‘˜è¦
    with open(task_summary_file, 'r', encoding='utf-8') as f:
        task_summary = json.load(f)
    
    # å°è¯•åŠ è½½æ¨¡å‹ç”»åƒï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    model_profiles = {}
    if os.path.exists(model_profiles_file):
        try:
            with open(model_profiles_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    model_profiles = json.loads(content)
        except (json.JSONDecodeError, ValueError):
            print(f"   âš ï¸  æ¨¡å‹ç”»åƒæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œå°†ä»è®°å½•é‡æ–°ç”Ÿæˆ")
            model_profiles = {}
    
    # å¦‚æœæ¨¡å‹ç”»åƒä¸ºç©ºï¼Œä»è®°å½•é‡æ–°ç”Ÿæˆ
    if not model_profiles:
        print("\nğŸ“Š ä»è®°å½•é‡æ–°ç”Ÿæˆæ¨¡å‹ç”»åƒ...")
        from fly_eval_plus_plus.main import FLYEvalPlusPlus
        evaluator = FLYEvalPlusPlus()
        
        for model_name, model_records in records_by_model.items():
            # å°†å­—å…¸è½¬æ¢ä¸ºRecordå¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼Œåªä¿ç•™å¿…è¦å­—æ®µï¼‰
            # æˆ–è€…ç›´æ¥ä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œä¿®æ”¹generate_model_profileä½¿å…¶å…¼å®¹
            try:
                # å°è¯•ä½¿ç”¨evaluatorç”Ÿæˆæ¨¡å‹ç”»åƒ
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¿®æ”¹generate_model_profileä»¥æ”¯æŒå­—å…¸æ ¼å¼
                # æš‚æ—¶è·³è¿‡ï¼Œä½¿ç”¨ç®€åŒ–çš„ç»Ÿè®¡ä¿¡æ¯
                eligible_count = sum(1 for r in model_records if r.get('agent_output', {}).get('adjudication') == 'eligible')
                total_scores = [r.get('optional_scores', {}).get('total_score', 0) for r in model_records if r.get('optional_scores', {}).get('total_score') is not None]
                avg_score = sum(total_scores) / len(total_scores) if total_scores else 0.0
                
                model_profiles[model_name] = {
                    "model_name": model_name,
                    "task_id": task_id,
                    "total_samples": len(model_records),
                    "eligible_samples": eligible_count,
                    "eligibility_rate": (eligible_count / len(model_records) * 100) if model_records else 0.0,
                    "average_overall_score": avg_score
                }
            except Exception as e:
                print(f"   âš ï¸  ç”Ÿæˆæ¨¡å‹ç”»åƒå¤±è´¥ ({model_name}): {e}")
                model_profiles[model_name] = {
                    "model_name": model_name,
                    "task_id": task_id,
                    "total_samples": len(model_records),
                    "eligible_samples": 0,
                    "eligibility_rate": 0.0,
                    "average_overall_score": 0.0
                }
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç›®å½•å¹¶ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹ç»“æœ...")
    for model_name, model_records in records_by_model.items():
        # åˆ›å»ºæ¨¡å‹ç›®å½•ï¼ˆæ¸…ç†æ¨¡å‹åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_model_name = model_name.replace('/', '_').replace('\\', '_')
        model_dir = new_base_dir / safe_model_name
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¥æ¨¡å‹çš„è®°å½•ï¼ˆJSONLæ ¼å¼ï¼‰
        records_file_model = model_dir / "records.jsonl"
        with open(records_file_model, 'w', encoding='utf-8') as f:
            for record in model_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        # ä¿å­˜è¯¥æ¨¡å‹çš„è®°å½•ï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
        records_file_model_json = model_dir / "records.json"
        with open(records_file_model_json, 'w', encoding='utf-8') as f:
            json.dump(model_records, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¥æ¨¡å‹çš„ç”»åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if model_name in model_profiles:
            profile_file = model_dir / "model_profile.json"
            with open(profile_file, 'w', encoding='utf-8') as f:
                json.dump(model_profiles[model_name], f, indent=2, ensure_ascii=False)
        
        # è®¡ç®—æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
        eligible_count = sum(1 for r in model_records if r.get('agent_output', {}).get('adjudication') == 'eligible')
        total_scores = [r.get('optional_scores', {}).get('total_score', 0) for r in model_records if r.get('optional_scores', {}).get('total_score') is not None]
        avg_score = sum(total_scores) / len(total_scores) if total_scores else 0.0
        
        model_summary = {
            "model_name": model_name,
            "task_id": task_id,
            "total_samples": len(model_records),
            "eligible_samples": eligible_count,
            "eligibility_rate": (eligible_count / len(model_records) * 100) if model_records else 0.0,
            "average_score": avg_score,
            "score_range": [min(total_scores), max(total_scores)] if total_scores else [0, 0]
        }
        
        summary_file = model_dir / "model_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(model_summary, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… {model_name}: {len(model_records)}æ¡è®°å½•, å¹³å‡åˆ†{avg_score:.2f}, Eligibleç‡{model_summary['eligibility_rate']:.2f}%")
    
    # ä¿å­˜ä»»åŠ¡çº§åˆ«çš„æ±‡æ€»æ–‡ä»¶
    print("\nğŸ’¾ ä¿å­˜ä»»åŠ¡çº§åˆ«æ±‡æ€»...")
    task_summary_file_new = new_base_dir / "task_summary.json"
    with open(task_summary_file_new, 'w', encoding='utf-8') as f:
        json.dump(task_summary, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹ç”»åƒ
    model_profiles_file_new = new_base_dir / "model_profiles.json"
    with open(model_profiles_file_new, 'w', encoding='utf-8') as f:
        json.dump(model_profiles, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    report_file = new_base_dir / "evaluation_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# è¯„ä¼°ç»“æœæŠ¥å‘Š - {task_id}\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {Path(__file__).stat().st_mtime}\n\n")
        f.write(f"## ä»»åŠ¡çº§åˆ«ç»Ÿè®¡\n\n")
        f.write(f"- **æ€»æ ·æœ¬æ•°**: {task_summary.get('total_samples', 0)}\n")
        f.write(f"- **æ€»æ¨¡å‹æ•°**: {len(records_by_model)}\n")
        f.write(f"- **Eligibleæ ·æœ¬æ•°**: {task_summary.get('eligible_samples', 0)}\n")
        f.write(f"- **Eligibleç‡**: {task_summary.get('eligibility_rate', 0.0):.2f}%\n")
        f.write(f"- **å¯ç”¨ç‡**: {task_summary.get('availability_rate', 0.0):.2f}%\n\n")
        
        f.write(f"## æ¨¡å‹æ’åï¼ˆæŒ‰å¹³å‡åˆ†ï¼‰\n\n")
        model_summaries = []
        for model_name, model_records in records_by_model.items():
            total_scores = [r.get('optional_scores', {}).get('total_score', 0) for r in model_records if r.get('optional_scores', {}).get('total_score') is not None]
            avg_score = sum(total_scores) / len(total_scores) if total_scores else 0.0
            eligible_count = sum(1 for r in model_records if r.get('agent_output', {}).get('adjudication') == 'eligible')
            model_summaries.append({
                'model_name': model_name,
                'avg_score': avg_score,
                'eligible_count': eligible_count,
                'total_samples': len(model_records)
            })
        
        model_summaries.sort(key=lambda x: x['avg_score'], reverse=True)
        
        f.write("| æ’å | æ¨¡å‹åç§° | æ ·æœ¬æ•° | å¹³å‡åˆ† | Eligibleæ•° | Eligibleç‡ |\n")
        f.write("|------|----------|--------|--------|------------|------------|\n")
        for rank, summary in enumerate(model_summaries, 1):
            eligible_rate = (summary['eligible_count'] / summary['total_samples'] * 100) if summary['total_samples'] > 0 else 0.0
            f.write(f"| {rank} | {summary['model_name']} | {summary['total_samples']} | "
                   f"{summary['avg_score']:.2f} | {summary['eligible_count']} | {eligible_rate:.2f}% |\n")
    
    print(f"\nâœ… ç»“æœå·²é‡æ–°ç»„ç»‡åˆ°: {new_base_dir}")
    print(f"   ç›®å½•ç»“æ„:")
    print(f"   {new_base_dir}/")
    print(f"   â”œâ”€â”€ task_summary.json")
    print(f"   â”œâ”€â”€ model_profiles.json")
    print(f"   â”œâ”€â”€ evaluation_report.md")
    print(f"   â””â”€â”€ [model_name]/")
    print(f"       â”œâ”€â”€ records.jsonl")
    print(f"       â”œâ”€â”€ records.json")
    print(f"       â”œâ”€â”€ model_profile.json")
    print(f"       â””â”€â”€ model_summary.json")


def fix_task_summary(records_file: str, task_summary_file: str):
    """ä¿®å¤ä»»åŠ¡æ‘˜è¦ä¸­çš„eligibility_rateè®¡ç®—é”™è¯¯"""
    print("\n" + "=" * 80)
    print("ä¿®å¤ä»»åŠ¡æ‘˜è¦")
    print("=" * 80)
    
    # åŠ è½½è®°å½•
    with open(records_file, 'r', encoding='utf-8') as f:
        records = [json.loads(line) for line in f]
    
    # é‡æ–°è®¡ç®—eligibility_rate
    total_samples = len(records)
    eligible_samples = sum(1 for r in records if r.get('agent_output', {}).get('adjudication') == 'eligible')
    eligibility_rate = (eligible_samples / total_samples * 100) if total_samples > 0 else 0.0
    
    # åŠ è½½ç°æœ‰ä»»åŠ¡æ‘˜è¦
    with open(task_summary_file, 'r', encoding='utf-8') as f:
        task_summary = json.load(f)
    
    # æ›´æ–°eligibility_rate
    task_summary['eligibility_rate'] = eligibility_rate
    task_summary['total_samples'] = total_samples
    task_summary['eligible_samples'] = eligible_samples
    task_summary['ineligible_samples'] = total_samples - eligible_samples
    
    # ä¿å­˜ä¿®å¤åçš„ä»»åŠ¡æ‘˜è¦
    with open(task_summary_file, 'w', encoding='utf-8') as f:
        json.dump(task_summary, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… ä¿®å¤å®Œæˆ:")
    print(f"      - æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"      - Eligibleæ ·æœ¬æ•°: {eligible_samples}")
    print(f"      - Eligibleç‡: {eligibility_rate:.2f}%")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æ£€æŸ¥å¹¶é‡æ–°ç»„ç»‡è¯„ä¼°ç»“æœ")
    parser.add_argument("--task", type=str, required=True, choices=["S1", "M1", "M3"], help="ä»»åŠ¡ID")
    parser.add_argument("--results_dir", type=str, default="./results/deterministic", help="ç»“æœç›®å½•")
    parser.add_argument("--check_only", action="store_true", help="ä»…æ£€æŸ¥ï¼Œä¸é‡æ–°ç»„ç»‡")
    parser.add_argument("--fix_only", action="store_true", help="ä»…ä¿®å¤ï¼Œä¸é‡æ–°ç»„ç»‡")
    
    args = parser.parse_args()
    
    task_id = args.task
    results_dir = Path(args.results_dir)
    
    # æ–‡ä»¶è·¯å¾„
    records_file = results_dir / f"records_{task_id}_deterministic.jsonl"
    task_summary_file = results_dir / f"task_summary_{task_id}_deterministic.json"
    model_profiles_file = results_dir / f"model_profiles_{task_id}_deterministic.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not records_file.exists():
        print(f"âŒ è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {records_file}")
        return
    
    if not task_summary_file.exists():
        print(f"âŒ ä»»åŠ¡æ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨: {task_summary_file}")
        return
    
    if not model_profiles_file.exists():
        print(f"âŒ æ¨¡å‹ç”»åƒæ–‡ä»¶ä¸å­˜åœ¨: {model_profiles_file}")
        return
    
    # æ£€æŸ¥ç»“æœè´¨é‡
    quality_ok = check_results_quality(
        str(records_file),
        str(task_summary_file),
        str(model_profiles_file)
    )
    
    if args.check_only:
        return
    
    # ä¿®å¤ä»»åŠ¡æ‘˜è¦
    if not quality_ok or args.fix_only:
        fix_task_summary(str(records_file), str(task_summary_file))
        if args.fix_only:
            return
    
    # é‡æ–°ç»„ç»‡ç»“æœ
    reorganize_results(
        str(results_dir),
        task_id,
        str(records_file),
        str(task_summary_file),
        str(model_profiles_file)
    )


if __name__ == "__main__":
    main()
