#!/usr/bin/env python3
"""
ç¡®å®šæ€§è¯„ä¼°ç³»ç»Ÿï¼ˆä¸ä½¿ç”¨å¤§æ¨¡å‹ï¼‰
åŸºäºè§„åˆ™å’Œç¡®å®šæ€§ç®—å­è¿›è¡Œè§„æ¨¡åŒ–è¯„ä¼°S1/M1/M3ä»»åŠ¡çš„æ‰€æœ‰æ¨¡å‹å›å¤

ä½¿ç”¨æ–¹æ³•:
    python run_deterministic_evaluation.py --task S1 --output_dir ./results/deterministic
    python run_deterministic_evaluation.py --task M1 --output_dir ./results/deterministic
    python run_deterministic_evaluation.py --task M3 --output_dir ./results/deterministic
    python run_deterministic_evaluation.py --all_tasks --output_dir ./results/deterministic
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from tqdm import tqdm
from datetime import datetime
import hashlib

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from fly_eval_plus_plus.main import FLYEvalPlusPlus
from fly_eval_plus_plus.core.data_structures import Sample, ModelOutput, ModelConfidence
from fly_eval_plus_plus.data_loader import DataLoader
from fly_eval_plus_plus.fusion.rule_based_fusion_aligned import RuleBasedFusionAligned


class DeterministicEvaluator:
    """
    ç¡®å®šæ€§è¯„ä¼°å™¨ï¼ˆä¸ä½¿ç”¨LLMï¼‰
    åŸºäºè§„åˆ™å’Œç¡®å®šæ€§ç®—å­è¿›è¡Œè¯„ä¼°
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        # åˆ›å»ºFLYEvalPlusPluså®ä¾‹ï¼Œä½†å¼ºåˆ¶ä½¿ç”¨rule_based fusion
        self.evaluator = FLYEvalPlusPlus(config_path=config_path)
        
        # å¼ºåˆ¶ä½¿ç”¨rule_based fusionï¼ˆä¸ä½¿ç”¨LLMï¼Œä½†ä¸LLM Judgeç‰ˆæœ¬å¯¹é½ï¼‰
        self.evaluator.fusion = RuleBasedFusionAligned(self.evaluator.config.fusion_protocol)
        
        # æ•°æ®åŠ è½½å™¨
        self.data_loader = DataLoader()
        
        print("âœ… ç¡®å®šæ€§è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼ˆä¸ä½¿ç”¨LLMï¼‰")
        print(f"   - ä½¿ç”¨Fusionç±»å‹: RuleBasedFusion")
        print(f"   - Verifieræ•°é‡: {len(self.evaluator.verifier_graph.verifiers)}")
    
    def evaluate_all_models(
        self,
        task_id: str,
        model_output_dir: str,
        reference_data_dir: str,
        confidence_data_dir: Optional[str] = None,
        output_dir: str = "./results/deterministic"
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        
        Args:
            task_id: ä»»åŠ¡ID (S1/M1/M3)
            model_output_dir: æ¨¡å‹è¾“å‡ºç›®å½•
            reference_data_dir: å‚è€ƒæ•°æ®ç›®å½•
            confidence_data_dir: ç½®ä¿¡åº¦æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\n{'='*80}")
        print(f"å¼€å§‹è¯„ä¼°ä»»åŠ¡: {task_id}")
        print(f"{'='*80}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
        
        # è·å–æ‰€æœ‰æ¨¡å‹åç§°
        model_output_path = Path(model_output_dir)
        if not model_output_path.exists():
            print(f"   âŒ æ¨¡å‹è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {model_output_dir}")
            return None
        
        model_dirs = [d for d in model_output_path.iterdir() if d.is_dir()]
        model_names = [d.name for d in model_dirs]
        print(f"   - æ‰¾åˆ° {len(model_names)} ä¸ªæ¨¡å‹")
        
        # åŠ è½½ç½®ä¿¡åº¦æ•°æ®ï¼ˆå¯é€‰ï¼Œç»Ÿä¸€åŠ è½½ï¼‰
        model_confidence_dict = {}
        if confidence_data_dir:
            try:
                model_confidence_dict = self.data_loader.load_model_confidence()
                print(f"   - ç½®ä¿¡åº¦æ•°æ®: {len(model_confidence_dict)}ä¸ªæ¨¡å‹")
            except Exception as e:
                print(f"   âš ï¸  åŠ è½½ç½®ä¿¡åº¦æ•°æ®å¤±è´¥: {e}")
        
        # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        all_records = []
        model_summaries = {}
        
        for model_name in tqdm(model_names, desc=f"è¯„ä¼°{task_id}ä»»åŠ¡", unit="æ¨¡å‹"):
            print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name}")
            
            # ä½¿ç”¨DataLoaderåˆ›å»ºsampleså’Œmodel_outputs
            try:
                samples, model_outputs = self.data_loader.create_samples_and_outputs(
                    task_id=task_id,
                    model_name=model_name
                )
            except Exception as e:
                print(f"   âš ï¸  åŠ è½½æ¨¡å‹æ•°æ®å¤±è´¥: {e}")
                continue
            
            if not samples or not model_outputs:
                print(f"   âš ï¸  æ¨¡å‹ {model_name} æ— æ•°æ®")
                continue
            
            print(f"   - æ ·æœ¬æ•°: {len(samples)}")
            
            # åŠ è½½ç½®ä¿¡åº¦æ•°æ®ï¼ˆå¯é€‰ï¼‰
            model_confidence = None
            if confidence_data_dir:
                try:
                    confidence_dict = self.data_loader.load_model_confidence()
                    if model_name in confidence_dict:
                        model_confidence = confidence_dict[model_name]
                except:
                    pass
            
            model_records = []
            model_scores = []
            
            # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
            for i, sample in enumerate(tqdm(samples, desc=f"  {model_name}", leave=False, unit="æ ·æœ¬")):
                # è·å–å¯¹åº”çš„æ¨¡å‹è¾“å‡º
                if i >= len(model_outputs):
                    continue
                
                model_output = model_outputs[i]
                
                # è¯„ä¼°æ ·æœ¬
                try:
                    record = self.evaluator.evaluate_sample(
                        sample=sample,
                        model_output=model_output,
                        model_confidence=model_confidence
                    )
                    model_records.append(record)
                    
                    # æ”¶é›†åˆ†æ•°
                    if record.optional_scores:
                        total_score = record.optional_scores.get('total_score', 0)
                        if total_score is not None:
                            model_scores.append(float(total_score))
                
                except Exception as e:
                    print(f"    âš ï¸  æ ·æœ¬ {sample.sample_id} è¯„ä¼°å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜æ¨¡å‹è®°å½•
            if model_records:
                all_records.extend(model_records)
                
                # è®¡ç®—æ¨¡å‹ç»Ÿè®¡
                avg_score = sum(model_scores) / len(model_scores) if model_scores else 0
                
                # ç»Ÿè®¡eligibleæ ·æœ¬ï¼ˆagent_outputæ˜¯dictï¼‰
                eligible_count = 0
                for r in model_records:
                    if hasattr(r, 'agent_output'):
                        # Recordå¯¹è±¡ï¼Œagent_outputæ˜¯dict
                        adjudication = r.agent_output.get('adjudication', 'ineligible')
                    else:
                        # å·²ç»æ˜¯dict
                        adjudication = r.get('agent_output', {}).get('adjudication', 'ineligible')
                    if adjudication == "eligible":
                        eligible_count += 1
                
                eligible_rate = eligible_count / len(model_records) if model_records else 0
                
                model_summaries[model_name] = {
                    "model_name": model_name,
                    "task_id": task_id,
                    "sample_count": len(model_records),
                    "avg_score": avg_score,
                    "eligible_count": eligible_count,
                    "eligible_rate": eligible_rate,
                    "scores": model_scores
                }
                
                print(f"   âœ… å®Œæˆ: {len(model_records)}ä¸ªæ ·æœ¬, å¹³å‡åˆ†: {avg_score:.2f}, Eligibleç‡: {eligible_rate:.2%}")
        
        # ç”Ÿæˆä»»åŠ¡æ‘˜è¦
        task_summary = self.evaluator.generate_task_summary(
            task_id=task_id,
            records=all_records
        )
        
        # åŠ è½½ç½®ä¿¡åº¦æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        model_confidence_dict = {}
        if confidence_data_dir:
            try:
                model_confidence_dict = self.data_loader.load_model_confidence()
            except:
                pass
        
        # ç”Ÿæˆæ¨¡å‹ç”»åƒ
        model_profiles = {}
        for model_name, summary in model_summaries.items():
            model_confidence = None
            if model_name in model_confidence_dict:
                model_confidence = model_confidence_dict[model_name]
            
            # è¿‡æ»¤è¯¥æ¨¡å‹çš„è®°å½•ï¼ˆå…¼å®¹Recordå¯¹è±¡å’Œdictï¼‰
            model_records_filtered = []
            for r in all_records:
                if hasattr(r, 'model_name'):
                    if r.model_name == model_name:
                        model_records_filtered.append(r)
                else:
                    if r.get('model_name') == model_name:
                        model_records_filtered.append(r)
            
            profile = self.evaluator.generate_model_profile(
                records=model_records_filtered,
                model_confidence=model_confidence
            )
            model_profiles[model_name] = profile
        
        # ä¿å­˜ç»“æœ
        self._save_results(
            task_id=task_id,
            records=all_records,
            task_summary=task_summary,
            model_profiles=model_profiles,
            model_summaries=model_summaries,
            output_dir=output_dir
        )
        
        return {
            "task_id": task_id,
            "total_samples": len(samples),
            "total_models": len(model_summaries),
            "total_records": len(all_records),
            "model_summaries": model_summaries,
            "task_summary": task_summary,
            "model_profiles": model_profiles
        }
    
    def _save_results(
        self,
        task_id: str,
        records: List[Any],
        task_summary: Any,
        model_profiles: Dict[str, Any],
        model_summaries: Dict[str, Any],
        output_dir: str
    ):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
        
        # ä¿å­˜è®°å½•ï¼ˆJSONLæ ¼å¼ï¼Œå¢é‡ä¿å­˜ï¼‰
        records_file = os.path.join(output_dir, f"records_{task_id}_deterministic.jsonl")
        with open(records_file, 'w', encoding='utf-8') as f:
            for record in records:
                # è½¬æ¢ä¸ºå­—å…¸
                record_dict = self._record_to_dict(record)
                f.write(json.dumps(record_dict, ensure_ascii=False) + '\n')
        print(f"   âœ… è®°å½•æ–‡ä»¶: {records_file} ({len(records)}æ¡)")
        
        # ä¿å­˜ä»»åŠ¡æ‘˜è¦
        task_summary_file = os.path.join(output_dir, f"task_summary_{task_id}_deterministic.json")
        task_summary_dict = self._task_summary_to_dict(task_summary)
        with open(task_summary_file, 'w', encoding='utf-8') as f:
            json.dump(task_summary_dict, f, indent=2, ensure_ascii=False)
        print(f"   âœ… ä»»åŠ¡æ‘˜è¦: {task_summary_file}")
        
        # ä¿å­˜æ¨¡å‹ç”»åƒ
        model_profiles_file = os.path.join(output_dir, f"model_profiles_{task_id}_deterministic.json")
        with open(model_profiles_file, 'w', encoding='utf-8') as f:
            profiles_dict = {
                model_name: self._model_profile_to_dict(profile)
                for model_name, profile in model_profiles.items()
            }
            json.dump(profiles_dict, f, indent=2, ensure_ascii=False)
        print(f"   âœ… æ¨¡å‹ç”»åƒ: {model_profiles_file}")
        
        # ä¿å­˜æ¨¡å‹æ‘˜è¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        model_summaries_file = os.path.join(output_dir, f"model_summaries_{task_id}_deterministic.json")
        with open(model_summaries_file, 'w', encoding='utf-8') as f:
            json.dump(model_summaries, f, indent=2, ensure_ascii=False)
        print(f"   âœ… æ¨¡å‹æ‘˜è¦: {model_summaries_file}")
        
        # ç”ŸæˆæŒ‡æ ‡æŠ¥å‘Š
        self._generate_metrics_report(
            task_id=task_id,
            model_summaries=model_summaries,
            task_summary=task_summary,
            output_dir=output_dir
        )
    
    def _record_to_dict(self, record: Any) -> Dict[str, Any]:
        """å°†Recordå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        # å¦‚æœå·²ç»æœ‰to_dictæ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(record, 'to_dict'):
            return record.to_dict()
        
        # å¦‚æœå·²ç»æ˜¯dictï¼Œç›´æ¥è¿”å›
        if isinstance(record, dict):
            return record
        
        # æ‰‹åŠ¨è½¬æ¢ï¼ˆå…¼å®¹Recordå¯¹è±¡ï¼‰
        protocol_result = record.protocol_result
        if isinstance(protocol_result, dict):
            protocol_dict = protocol_result
        else:
            # ProtocolResultå¯¹è±¡
            parsing = protocol_result.parsing if hasattr(protocol_result, 'parsing') else {}
            field_completeness = protocol_result.field_completeness if hasattr(protocol_result, 'field_completeness') else {}
            protocol_dict = {
                "parsing": {
                    "success": parsing.success if hasattr(parsing, 'success') else parsing.get('success'),
                    "error": parsing.error if hasattr(parsing, 'error') else parsing.get('error')
                },
                "field_completeness": {
                    "completeness_rate": field_completeness.completeness_rate if hasattr(field_completeness, 'completeness_rate') else field_completeness.get('completeness_rate'),
                    "missing_fields": field_completeness.missing_fields if hasattr(field_completeness, 'missing_fields') else field_completeness.get('missing_fields', [])
                }
            }
        
        evidence_pack = record.evidence_pack if hasattr(record, 'evidence_pack') else {}
        if isinstance(evidence_pack, dict):
            evidence_atoms = evidence_pack.get('atoms', [])
            # ç¡®ä¿æ‰€æœ‰atomséƒ½æ˜¯å­—ç¬¦ä¸²
            evidence_atoms = [str(atom) if not isinstance(atom, str) else atom for atom in evidence_atoms]
        else:
            evidence_atoms = [str(atom) for atom in evidence_pack.atoms] if hasattr(evidence_pack, 'atoms') else []
        
        agent_output = record.agent_output if hasattr(record, 'agent_output') else {}
        if isinstance(agent_output, dict):
            agent_dict = agent_output
        else:
            checklist = agent_output.checklist if hasattr(agent_output, 'checklist') else []
            agent_dict = {
                "checklist": [
                    {
                        "item_id": item.get("item_id") if isinstance(item, dict) else getattr(item, 'item_id', None),
                        "constraint_id": item.get("constraint_id") if isinstance(item, dict) else getattr(item, 'constraint_id', None),
                        "status": item.get("status") if isinstance(item, dict) else getattr(item, 'status', None),
                        "evidence_ids": item.get("evidence_ids", []) if isinstance(item, dict) else getattr(item, 'evidence_ids', [])
                    }
                    for item in checklist
                ],
                "adjudication": agent_output.adjudication if hasattr(agent_output, 'adjudication') else agent_output.get('adjudication', 'ineligible'),
                "attribution": agent_output.attribution if hasattr(agent_output, 'attribution') else agent_output.get('attribution', [])
            }
        
        trace = record.trace if hasattr(record, 'trace') else {}
        if isinstance(trace, dict):
            trace_dict = trace
        else:
            trace_dict = {
                "config_hash": getattr(trace, 'config_hash', None),
                "schema_version": getattr(trace, 'schema_version', None),
                "constraint_lib_version": getattr(trace, 'constraint_lib_version', None),
                "timestamp": getattr(trace, 'timestamp', None),
                "evaluator_version": getattr(trace, 'evaluator_version', None)
            }
        
        return {
            "sample_id": getattr(record, 'sample_id', None),
            "model_name": getattr(record, 'model_name', None),
            "task_id": getattr(record, 'task_id', None),
            "protocol_result": protocol_dict,
            "evidence_pack": {"atoms": evidence_atoms},
            "agent_output": agent_dict,
            "optional_scores": getattr(record, 'optional_scores', {}),
            "trace": trace_dict
        }
    
    def _task_summary_to_dict(self, task_summary: Any) -> Dict[str, Any]:
        """å°†TaskSummaryå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        if isinstance(task_summary, dict):
            return task_summary
        
        if hasattr(task_summary, 'to_dict'):
            return task_summary.to_dict()
        
        # æ‰‹åŠ¨è½¬æ¢
        return {
            "task_id": getattr(task_summary, 'task_id', None),
            "total_samples": getattr(task_summary, 'total_samples', 0),
            "eligible_samples": getattr(task_summary, 'eligible_samples', 0),
            "ineligible_samples": getattr(task_summary, 'ineligible_samples', 0),
            "compliance_rate": getattr(task_summary, 'compliance_rate', {}),
            "availability_rate": getattr(task_summary, 'availability_rate', 0.0),
            "eligibility_rate": getattr(task_summary, 'eligibility_rate', 0.0),
            "constraint_satisfaction_profile": getattr(task_summary, 'constraint_satisfaction_profile', {}),
            "failure_mode_distribution": getattr(task_summary, 'failure_mode_distribution', {}),
            "conditional_error_statistics": getattr(task_summary, 'conditional_error_statistics', {}),
            "tail_risks": getattr(task_summary, 'tail_risks', {})
        }
    
    def _model_profile_to_dict(self, model_profile: Any) -> Dict[str, Any]:
        """å°†ModelProfileå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        if isinstance(model_profile, dict):
            return model_profile
        
        if hasattr(model_profile, 'to_dict'):
            return model_profile.to_dict()
        
        # æ‰‹åŠ¨è½¬æ¢ï¼ˆä½¿ç”¨getattrå®‰å…¨è®¿é—®ï¼‰
        return {
            "model_name": getattr(model_profile, 'model_name', 'unknown'),
            "task_id": getattr(model_profile, 'task_id', None),
            "total_samples": getattr(model_profile, 'total_samples', 0),
            "eligible_samples": getattr(model_profile, 'eligible_samples', 0),
            "eligibility_rate": getattr(model_profile, 'eligibility_rate', 0.0),
            "availability_rate": getattr(model_profile, 'availability_rate', 0.0),
            "average_overall_score": getattr(model_profile, 'average_overall_score', 0.0),
            "constraint_satisfaction_profile": getattr(model_profile, 'constraint_satisfaction_profile', {}),
            "failure_mode_distribution": getattr(model_profile, 'failure_mode_distribution', {}),
            "conditional_error": getattr(model_profile, 'conditional_error', {}),
            "tail_risks": getattr(model_profile, 'tail_risks', {}),
            "model_confidence": getattr(model_profile, 'model_confidence', {})
        }
    
    def _generate_metrics_report(
        self,
        task_id: str,
        model_summaries: Dict[str, Any],
        task_summary: Any,
        output_dir: str
    ):
        """ç”ŸæˆæŒ‡æ ‡æŠ¥å‘Š"""
        report_file = os.path.join(output_dir, f"metrics_report_{task_id}_deterministic.md")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# ç¡®å®šæ€§è¯„ä¼°æŒ‡æ ‡æŠ¥å‘Š - {task_id}\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**è¯„ä¼°æ–¹æ³•**: ç¡®å®šæ€§è§„åˆ™å’Œç®—å­ï¼ˆä¸ä½¿ç”¨LLMï¼‰\n\n")
            
            # ä»»åŠ¡çº§åˆ«ç»Ÿè®¡
            f.write("## ä»»åŠ¡çº§åˆ«ç»Ÿè®¡\n\n")
            # å¤„ç†task_summaryå¯èƒ½æ˜¯dictçš„æƒ…å†µ
            if isinstance(task_summary, dict):
                total_samples = task_summary.get('total_samples', 0)
                availability_rate = task_summary.get('availability_rate', 0.0)
                eligibility_rate = task_summary.get('eligibility_rate', 0.0)
                compliance_rate = task_summary.get('compliance_rate', {})
            else:
                total_samples = getattr(task_summary, 'total_samples', 0)
                availability_rate = getattr(task_summary, 'availability_rate', 0.0)
                eligibility_rate = getattr(task_summary, 'eligibility_rate', 0.0)
                compliance_rate = getattr(task_summary, 'compliance_rate', {})
            
            f.write(f"- **æ€»æ ·æœ¬æ•°**: {total_samples}\n")
            f.write(f"- **æ€»æ¨¡å‹æ•°**: {len(model_summaries)}\n")
            if isinstance(compliance_rate, dict):
                f.write(f"- **åˆè§„ç‡**: è§å„çº¦æŸç±»å‹\n")
            else:
                f.write(f"- **åˆè§„ç‡**: {compliance_rate:.2%}\n")
            f.write(f"- **å¯ç”¨ç‡**: {availability_rate:.2%}\n")
            f.write(f"- **Eligibleç‡**: {eligibility_rate:.2%}\n\n")
            
            # æ¨¡å‹æ’å
            f.write("## æ¨¡å‹æ’åï¼ˆæŒ‰å¹³å‡åˆ†ï¼‰\n\n")
            sorted_models = sorted(
                model_summaries.items(),
                key=lambda x: x[1]['avg_score'],
                reverse=True
            )
            
            f.write("| æ’å | æ¨¡å‹åç§° | æ ·æœ¬æ•° | å¹³å‡åˆ† | Eligibleç‡ |\n")
            f.write("|------|----------|--------|--------|------------|\n")
            
            for rank, (model_name, summary) in enumerate(sorted_models, 1):
                f.write(
                    f"| {rank} | {model_name} | {summary['sample_count']} | "
                    f"{summary['avg_score']:.2f} | {summary['eligible_rate']:.2%} |\n"
                )
            
            f.write("\n")
            
            # è¯¦ç»†æŒ‡æ ‡
            f.write("## è¯¦ç»†æŒ‡æ ‡\n\n")
            for model_name, summary in sorted_models:
                f.write(f"### {model_name}\n\n")
                f.write(f"- **æ ·æœ¬æ•°**: {summary['sample_count']}\n")
                f.write(f"- **å¹³å‡åˆ†**: {summary['avg_score']:.2f}\n")
                f.write(f"- **Eligibleç‡**: {summary['eligible_rate']:.2%}\n")
                f.write(f"- **åˆ†æ•°èŒƒå›´**: {min(summary['scores']):.2f} - {max(summary['scores']):.2f}\n")
                f.write(f"- **åˆ†æ•°ä¸­ä½æ•°**: {sorted(summary['scores'])[len(summary['scores'])//2]:.2f}\n\n")
        
        print(f"   âœ… æŒ‡æ ‡æŠ¥å‘Š: {report_file}")


def main():
    parser = argparse.ArgumentParser(description="ç¡®å®šæ€§è¯„ä¼°ç³»ç»Ÿï¼ˆä¸ä½¿ç”¨å¤§æ¨¡å‹ï¼‰")
    parser.add_argument(
        "--task",
        type=str,
        choices=["S1", "M1", "M3"],
        help="ä»»åŠ¡ID (S1/M1/M3)"
    )
    parser.add_argument(
        "--all_tasks",
        action="store_true",
        help="è¯„ä¼°æ‰€æœ‰ä»»åŠ¡"
    )
    parser.add_argument(
        "--model_output_dir",
        type=str,
        default="../data/model_results",
        help="æ¨¡å‹è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--reference_data_dir",
        type=str,
        default="../data/reference_data",
        help="å‚è€ƒæ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--confidence_data_dir",
        type=str,
        default=None,
        help="ç½®ä¿¡åº¦æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼‰"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./results/deterministic",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
    )
    
    args = parser.parse_args()
    
    if not args.task and not args.all_tasks:
        parser.error("å¿…é¡»æŒ‡å®š --task æˆ– --all_tasks")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = DeterministicEvaluator(config_path=args.config)
    
    # ç¡®å®šè¦è¯„ä¼°çš„ä»»åŠ¡
    tasks = []
    if args.all_tasks:
        tasks = ["S1", "M1", "M3"]
    else:
        tasks = [args.task]
    
    # è¯„ä¼°æ¯ä¸ªä»»åŠ¡
    all_results = {}
    for task_id in tasks:
        print(f"\n{'='*80}")
        print(f"å¼€å§‹è¯„ä¼°ä»»åŠ¡: {task_id}")
        print(f"{'='*80}")
        
        # æ ¹æ®ä»»åŠ¡ç¡®å®šæ•°æ®è·¯å¾„ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        base_path = Path(__file__).parent.parent
        if task_id == "S1":
            task_model_dir = str(base_path / "data" / "model_results" / "S1_20251106_020205")
        elif task_id == "M1":
            # M1æ•°æ®å¯èƒ½åœ¨å¤–éƒ¨è·¯å¾„
            task_model_dir = str(base_path / "data" / "model_results" / "M1" / "20251107_155714")
            if not os.path.exists(task_model_dir):
                # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
                possible_paths = [
                    "../../model_invocation/results/M1/20251107_155714",
                    "../../../model_invocation/results/M1/20251107_155714",
                    str(Path(__file__).parent.parent.parent / "model_invocation" / "results" / "M1" / "20251107_155714")
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        task_model_dir = path
                        break
        elif task_id == "M3":
            # M3æ•°æ®å¯èƒ½åœ¨å¤–éƒ¨è·¯å¾„
            task_model_dir = str(base_path / "data" / "model_results" / "M3" / "20251108_155714")
            if not os.path.exists(task_model_dir):
                # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
                possible_paths = [
                    "../../model_invocation/results/M3/20251108_155714",
                    "../../../model_invocation/results/M3/20251108_155714",
                    str(Path(__file__).parent.parent.parent / "model_invocation" / "results" / "M3" / "20251108_155714")
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        task_model_dir = path
                        break
        else:
            print(f"âš ï¸  æœªçŸ¥ä»»åŠ¡: {task_id}")
            continue
        
        # å‚è€ƒæ•°æ®è·¯å¾„
        reference_data_dir = str(base_path / "data" / "reference_data")
        
        # è¯„ä¼°
        result = evaluator.evaluate_all_models(
            task_id=task_id,
            model_output_dir=task_model_dir,
            reference_data_dir=reference_data_dir,
            confidence_data_dir=args.confidence_data_dir,
            output_dir=args.output_dir
        )
        
        all_results[task_id] = result
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    print(f"{'='*80}")
    
    # è½¬æ¢all_resultsä¸­çš„TaskSummaryå¯¹è±¡ä¸ºå­—å…¸
    serializable_results = {}
    for task_id, result in all_results.items():
        if result is None:
            continue
        serializable_results[task_id] = {
            "task_id": result.get("task_id") if isinstance(result, dict) else getattr(result, "task_id", task_id),
            "total_samples": result.get("total_samples") if isinstance(result, dict) else getattr(result, "total_samples", 0),
            "total_models": result.get("total_models") if isinstance(result, dict) else len(result.get("model_summaries", {})) if isinstance(result, dict) else 0,
            "total_records": result.get("total_records") if isinstance(result, dict) else getattr(result, "total_records", 0),
            "model_summaries": result.get("model_summaries", {}) if isinstance(result, dict) else {},
            "task_summary": evaluator._task_summary_to_dict(result.get("task_summary")) if isinstance(result, dict) and result.get("task_summary") else {},
            "model_profiles": {k: evaluator._model_profile_to_dict(v) for k, v in result.get("model_profiles", {}).items()} if isinstance(result, dict) and result.get("model_profiles") else {}
        }
    
    summary_file = os.path.join(args.output_dir, "evaluation_summary_deterministic.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    print(f"âœ… ç»¼åˆæŠ¥å‘Š: {summary_file}")
    
    print(f"\n{'='*80}")
    print("âœ… æ‰€æœ‰è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()

