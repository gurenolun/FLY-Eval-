#!/usr/bin/env python3
"""
è¯„ä¼°merged_resultsä¸­çš„46ä¸ªæ¨¡å‹
ä½¿ç”¨ä¸å½“å‰21ä¸ªSOTAæ¨¡å‹ç›¸åŒçš„è¯„ä¼°æµç¨‹
"""

import json
import os
import sys
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
import statistics

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "fly_eval_plus_plus"))

from fly_eval_plus_plus.data_loader import DataLoader
from fly_eval_plus_plus.run_deterministic_evaluation import DeterministicEvaluator
from fly_eval_plus_plus.core.data_structures import Sample, ModelOutput

def run_evaluation():
    """
    è¿è¡Œ46ä¸ªæ¨¡å‹çš„è¯„ä¼°
    """
    print("="*80)
    print("è¯„ä¼°merged_resultsä¸­çš„46ä¸ªæ¨¡å‹")
    print("="*80)
    
    # é…ç½®
    MERGED_RESULTS_DIR = Path("data/model_results/merged_results_20250617_203957")
    OUTPUT_DIR = Path("results/all_46_models_v7_physics_fixed")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    TASK_ID = "S1"
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    print(f"\nğŸ“Š åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä»»åŠ¡: {TASK_ID}ï¼‰")
    det_evaluator = DeterministicEvaluator()
    evaluator = det_evaluator.evaluator  # ä½¿ç”¨å†…éƒ¨çš„FLYEvalPlusPluså®ä¾‹
    
    # åˆå§‹åŒ–DataLoader
    data_loader = DataLoader()
    
    # åŠ è½½reference data (ground truth)
    print("\nğŸ“‚ åŠ è½½reference data...")
    reference_data = data_loader.load_reference_data(TASK_ID)
    print(f"   âœ… åŠ è½½äº† {len(reference_data)} æ¡referenceæ•°æ®")
    
    # è·å–æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
    model_files = sorted([f for f in MERGED_RESULTS_DIR.glob("*.jsonl") 
                         if f.name != "flight_questions_temp.jsonl"])
    
    print(f"   æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
    
    # æŒ‰æ¨¡å‹è¯„ä¼°
    all_records = []
    model_summaries = {}
    
    for i, model_file in enumerate(model_files, 1):
        model_name = model_file.stem.replace('_test_results_' + model_file.stem.split('_')[-1], '')
        print(f"\nğŸ“Š [{i}/{len(model_files)}] è¯„ä¼°æ¨¡å‹: {model_name}")
        
        # åŠ è½½æ¨¡å‹æ•°æ®
        samples = []
        model_outputs = []
        
        sample_count = 0
        with open(model_file, 'r') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    record = json.loads(line)
                    
                    # æ„å»ºSampleï¼ˆä½¿ç”¨DataLoaderçš„é€»è¾‘ï¼‰
                    sample_id = record.get('id', f"{model_name}_{sample_count}")
                    question = record.get('question', '')
                    
                    # æå–current_stateï¼ˆç®€åŒ–ç‰ˆï¼‰
                    current_state = {}
                    try:
                        # å°è¯•ä»questionä¸­æå–ä¸Šä¸€ç§’æ•°æ®
                        import re
                        json_match = re.search(r'ä¸Šä¸€ç§’æ•°æ®[ï¼š:]\s*\n?(\{.*?\})', question, re.DOTALL)
                        if json_match:
                            current_state = json.loads(json_match.group(1))
                    except:
                        pass
                    
                    # è·å–gold (ground truth)
                    gold = {}
                    gold_available = False
                    if TASK_ID == "S1" and sample_count < len(reference_data):
                        ref_record = reference_data[sample_count]
                        gold = {
                            "next_second": ref_record.get('next_second', {}),
                            "available": True
                        }
                        gold_available = True
                    
                    sample = Sample(
                        sample_id=sample_id,
                        task_id=TASK_ID,
                        context={
                            "question": question,
                            "current_state": current_state,
                            "record_idx": sample_count
                        },
                        gold=gold if gold_available else {"available": False}
                    )
                    
                    # è§£æresponseä¸ºJSON
                    response_text = record.get('response', '')
                    try:
                        parsed_data = json.loads(response_text)
                    except json.JSONDecodeError:
                        # å°è¯•ä»markdownæå–
                        import re
                        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                        if json_match:
                            try:
                                parsed_data = json.loads(json_match.group(1))
                            except:
                                parsed_data = {}
                        else:
                            parsed_data = {}
                    
                    model_output = ModelOutput(
                        model_name=record.get('model', model_name),
                        sample_id=sample_id,
                        raw_response_text=response_text,
                        timestamp=record.get('timestamp', ''),
                        task_id=TASK_ID
                    )
                    
                    samples.append(sample)
                    model_outputs.append(model_output)
                    sample_count += 1
                    
                except Exception as e:
                    print(f"   âš ï¸  è·³è¿‡æ ·æœ¬: {e}")
                    continue
        
        print(f"   - åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
        
        if len(samples) == 0:
            print(f"   âŒ æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
            continue
        
        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        model_records = []
        model_scores = []
        dimension_scores_list = defaultdict(list)
        
        for sample, model_output in tqdm(zip(samples, model_outputs), 
                                         total=len(samples),
                                         desc=f"  {model_name}",
                                         leave=False,
                                         unit="æ ·æœ¬"):
            try:
                record = evaluator.evaluate_sample(
                    sample=sample,
                    model_output=model_output,
                    model_confidence=None
                )
                model_records.append(record)
                
                # æ”¶é›†åˆ†æ•°
                if record.optional_scores:
                    dim_scores = record.optional_scores.get('dimension_scores', {})
                    total_score = record.optional_scores.get('total_score', 0)
                    
                    model_scores.append(total_score)
                    
                    # æ”¶é›†å„ç»´åº¦åˆ†æ•°
                    for dim, score in dim_scores.items():
                        dimension_scores_list[dim].append(score * 100)  # è½¬ä¸ºç™¾åˆ†æ¯”
                
            except Exception as e:
                print(f"\n   âš ï¸  è¯„ä¼°æ ·æœ¬ {sample.sample_id} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        all_records.extend(model_records)
        
        # è®¡ç®—æ¨¡å‹æ‘˜è¦
        if model_scores:
            avg_score = statistics.mean(model_scores)
            
            # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
            dim_avg = {}
            for dim, scores in dimension_scores_list.items():
                dim_avg[dim] = statistics.mean(scores)
            
            model_summaries[model_name] = {
                'avg_total_score': avg_score,
                'num_samples': len(model_scores),
                'num_errors': len(samples) - len(model_scores),
                'dimension_scores': dim_avg
            }
            
            print(f"   âœ… å¹³å‡æ€»åˆ†: {avg_score:.2f} ({len(model_scores)}/{len(samples)} æ ·æœ¬æˆåŠŸ)")
            print(f"      Protocol: {dim_avg.get('protocol_schema_compliance', 0):.1f}%, " +
                  f"Field: {dim_avg.get('field_validity_local_dynamics', 0):.1f}%, " +
                  f"Physics: {dim_avg.get('physics_cross_field_consistency', 0):.1f}%, " +
                  f"Safety: {dim_avg.get('safety_constraint_satisfaction', 0):.1f}%, " +
                  f"Pred: {dim_avg.get('predictive_quality_reliability', 0):.1f}%")
        else:
            print(f"   âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ ·æœ¬")
            model_summaries[model_name] = {
                'avg_total_score': 0,
                'num_samples': 0,
                'num_errors': len(samples),
                'dimension_scores': {}
            }
    
    # ä¿å­˜ç»“æœ
    print("\n" + "="*80)
    print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ")
    print("="*80)
    
    # ä¿å­˜æ‰€æœ‰è®°å½•
    records_file = OUTPUT_DIR / f"records_{TASK_ID}_all_46_models.jsonl"
    with open(records_file, 'w') as f:
        for record in all_records:
            # è½¬æ¢ä¸ºdict
            rec_dict = {
                'sample_id': record.sample_id,
                'task_id': record.task_id,
                'model_name': record.model_name,
                'protocol_result': record.protocol_result.__dict__ if hasattr(record.protocol_result, '__dict__') else record.protocol_result,
                'evidence_pack': {
                    'atoms': [str(atom) for atom in record.evidence_pack.atoms],
                    'metadata': record.evidence_pack.metadata
                },
                'optional_scores': record.optional_scores,
                'agent_output': record.agent_output,
                'trace': record.trace
            }
            f.write(json.dumps(rec_dict, default=str) + '\n')
    
    print(f"âœ… ä¿å­˜è®°å½•: {records_file} ({len(all_records)} æ¡)")
    
    # ä¿å­˜æ¨¡å‹æ‘˜è¦
    summary_file = OUTPUT_DIR / "model_summaries.json"
    with open(summary_file, 'w') as f:
        json.dump(model_summaries, f, indent=2, ensure_ascii=False)
    print(f"âœ… ä¿å­˜æ‘˜è¦: {summary_file}")
    
    # æ‰“å°æ’è¡Œæ¦œ
    print("\n" + "="*80)
    print("ğŸ† Top 20 æ¨¡å‹ï¼ˆæŒ‰æ€»åˆ†æ’åºï¼‰")
    print("="*80)
    
    sorted_models = sorted(model_summaries.items(), 
                          key=lambda x: x[1]['avg_total_score'], 
                          reverse=True)
    
    print(f"\n{'æ’å':<4} {'æ¨¡å‹':50s} {'æ€»åˆ†':>8} {'Proto':>7} {'Field':>7} {'Phys':>7} {'Safety':>7} {'Pred':>7}")
    print("-" * 120)
    
    for i, (model, summary) in enumerate(sorted_models[:20], 1):
        dim_scores = summary.get('dimension_scores', {})
        print(f"{i:<4} {model:50s} {summary['avg_total_score']:7.2f}  " +
              f"{dim_scores.get('protocol_schema_compliance', 0):6.1f}% " +
              f"{dim_scores.get('field_validity_local_dynamics', 0):6.1f}% " +
              f"{dim_scores.get('physics_cross_field_consistency', 0):6.1f}% " +
              f"{dim_scores.get('safety_constraint_satisfaction', 0):6.1f}% " +
              f"{dim_scores.get('predictive_quality_reliability', 0):6.1f}%")
    
    print("\n" + "="*80)
    print(f"âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"   æ€»è®°å½•æ•°: {len(all_records)}")
    print(f"   æˆåŠŸè¯„ä¼°çš„æ¨¡å‹æ•°: {sum(1 for s in model_summaries.values() if s['num_samples'] > 0)}/{len(model_summaries)}")
    print("="*80)
    
    return all_records, model_summaries

if __name__ == "__main__":
    run_evaluation()
